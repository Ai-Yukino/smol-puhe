---
title: "AudioProj Report"
author: "TTS R Group"
date: '2022-06-14'
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# The Relationship between Speech Acoustics and Gender

**Our Hypothesis**: Sounds where $f_1 - f_0$ or $f_1 / f_0$ are larger are more likely to come from female speakers.

## R Packages/ Libraries

```{r message=FALSE}
library(caret)
library(tidyverse)
library(factoextra)
library(cluster)
library(ggplot2)
library(e1071)
library(tidyr)
library(skimr)
library(readxl)
library(kernlab)
library(randomForest)
```

## Set working directory for R Studio

```{r}
wd <- getwd() %>% str_replace("/scripts/R/praat_processing$", "")
setwd(wd)
getwd()
rm(wd)

data <- read_csv("./data/filtered_data.csv")
spec(data)
```

## Supervised Model: Caret Prediction

```{r}
data<- read_excel("./data/filtered_data.xls")
dim(data)

df <- data[c("f1", "f2", "f3", "f4", "f0", "intensity", "harmonicity", "age","gender")]
head(df)

# dimensions of data
dim(df)


#create output as target class
df$gender = as.factor(df$gender)

# Step 1: Get row numbers for the training data
trainRowNumbers <- createDataPartition(df$gender, p=0.8, list=FALSE)

# Step 2: Create the training  data
trainData <- df[trainRowNumbers,]

# Step 3: Create the test data
testData <- df[-trainRowNumbers,]

# Store X and Y for later use.
x = trainData[ 1:5]
y = factor(trainData[[9]])


Sys.setlocale( locale = 'Chinese')
#skimmed <- skim_to_wide(trainData)
#skimmed[,]
skim(df)

df %>% dplyr::group_by(gender) %>% skim()

#create a list of 80% of rows for training
validation_index <- createDataPartition(df$gender, p=0.80, list=FALSE)
validation <- df[-validation_index,]
df <- df[validation_index,]


# dimensions of data
dim(df)

#summarize attribute distributions
summary(df)

#summarize the class distribution
percentage <-prop.table(table(df$gender)) *100
y = factor(trainData[[9]])
cbind(freq=table(df$gender), percentage =percentage)

#split input and output
#x<-df[,1:5]
#y<-as.factor(df[[9]])

#boxplot for each attribute
par(mfrow=c(1,5))
  for(i in 1:5){
    boxplot(x[,i], main=names(df)[i])
  }


# Run algorithms using 10-fold cross validation
control <- trainControl(method="cv", number=10)
metric <- "Accuracy"

# a) linear algorithms
set.seed(7)
fit.lda <- train(gender~., data=df, method="lda", metric=metric, trControl=control)
# b) nonlinear algorithms
# CART
set.seed(7)
fit.cart <- train(gender~., data=df, method="rpart", metric=metric, trControl=control)
# kNN
set.seed(7)
fit.knn <- train(gender~., data=df, method="knn", metric=metric, trControl=control)
# c) advanced algorithms
# SVM
set.seed(7)
fit.svm <- train(gender~., data=df, method="svmRadial", metric=metric, trControl=control)
# Random Forest
set.seed(7)
fit.rf <- train(gender~., data=df, method="rf", metric=metric, trControl=control)

#summarize accuracy of models
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)

# compare accuracy of models
dotplot(results)

# best model is random forest
print(fit.rf)

#analyze results in confusion matrix

# estimate skill of rf on the validation data
predictions <- predict(fit.rf, validation)
confusionMatrix(predictions, validation$gender)
```

### Supervised Learning: Classification

```{r}
# Selecting specific columns 
data <- data[, c('f0', 'f1', 'f2', 'gender')]
head(data)

# Splitting data

library(caTools)

sample.split(data$gender,SplitRatio = 0.65)->split_values
subset(data,split_values==T)->train_set
subset(data,split_values==F)->test_set

# Loading RPART to build classification model
library(rpart)

rpart(gender ~ ., data = train_set)->mod_class
predict(mod_class, test_set, type = "matrix" )->result_class
table(test_set$gender, result_class)

library(ggplot2)

data <- read_csv("./data/filtered_data.csv")

plot(data$gender, data$f0,
     main = "Formants by Speaker's Gender",
     xlab = "Speaker's Gender",
     ylab = "Formants",
     type = "l",
     ylim = c(50, 3000))
lines(data$gender, data$f1,
      lty = "dashed")
lines(data$gender, data$f2,
      lty = "dotted")
```

## Unsupervised Learning: Principal Components Analysis

```{r}
data <- data[, c('f0', 'f1', 'f2', 'gender')]

#Calculate the Principal Components
results <- prcomp(data, scale = TRUE)

#reverse the signs
results$rotation <- -1*results$rotation

#display principal components
results$rotation

#reverse the signs of the scores
results$x <- -1*results$x

#display the first six scores
head(results$x)

biplot(results, scale = 0)

#calculate total variance explained by each principal component
results$sdev^2 / sum(results$sdev^2)

#calculate total variance explained by each principal component
var_explained = results$sdev^2 / sum(results$sdev^2)

#create scree plot
qplot(c(1:4), var_explained) + 
  geom_line() + 
  xlab("Principal Component") + 
  ylab("Variance Explained") +
  ggtitle("Scree Plot") +
  ylim(0, 1)
```

## Kmeans()

```{r}
data <- read_csv("./data/filtered_data.csv")

data$difference = data$f1 - data$f0
data$quotient = data$f1/data$f0
dataMod = subset(data, select = -c(b1,b2,b3,b4,f1p,f2p,f3p,f4p, intensity, harmonicity, ...1, time))

model1 = lm(difference~gender, data = data)
summary(model1)
model2 = lm(quotient~gender, data = data)
summary(model2)


model3 = lm(difference~., data = subset(dataMod, select = -c(quotient, f0, f1)))
summary(model3)
model4 = lm(quotient~., data = subset(dataMod, select = -c(difference, f0, f1)))
summary(model4)

# fviz_nbclust(dataMod, kmeans, method = "wss")
# gapStat = clusGap(dataMod, FUN = kmeans, K.max = 10, B = 50)
# fviz_gap_stat(gapStat)

km1 = kmeans(dataMod, centers = 2)
km1

km2 = kmeans(dataMod, centers = 6)
km2

km3 = kmeans(dataMod, centers = 8)
km3

fviz_cluster(km1, dataMod)
fviz_cluster(km2, dataMod)
fviz_cluster(km3, dataMod)


# fviz_nbclust(subset(dataMod, select = c(f1, f2, gender)), kmeans, method = "wss")
# gapStat2 = clusGap(subset(dataMod, select = c(f1, f2, gender)), FUN = kmeans, K.max = 10, B = 50)
# fviz_gap_stat(gapStat2)

km4 = kmeans(dataMod, centers = 2)
km4

km5 = kmeans(dataMod, centers = 4)
km5

km6 = kmeans(dataMod, centers = 5)
km6

fviz_cluster(km4, subset(dataMod, select = c(f1, f2, gender)))
fviz_cluster(km5, subset(dataMod, select = c(f1, f2, gender)))
fviz_cluster(km6, subset(dataMod, select = c(f1, f2, gender)))

ggplot(dataMod, aes(x = f0, y = f1)) + geom_point(aes(color = gender, alpha = 0.01))


fullMod = lm(gender~f0 + f1 + f2, dataMod)
redMod = lm(gender~f0, dataMod)
anova(redMod, fullMod)

test = lm(gender~f0, dataMod)
summary(test)

test2 = lm(gender~f0 + f1, dataMod)
summary(test2)

test3 = lm(gender~f0 + f1 + f0*f1, dataMod)
summary(test3)
```

## Logistic Regression

```{r}
## Logistic regression 
## gender ~ f0

ggplot(data, aes(x=f0, y=gender)) +
  geom_point(alpha=.5, aes(color = cut(data$gender, c(-1, 0.5, 1)))) +
  scale_color_manual(name = "Gender",
                     values = c("(-1,0.5]" = "#ff0080",
                                "(0.5,1]" = "#0000ff"),
                     labels = c("Female", "Male")) +
  stat_smooth(method="glm", se=FALSE,
              method.args = list(family=binomial), col="#a349a4") +
  labs(title = "Logistic regression",
       subtitle = "gender ~ f0",
       caption = "Data: smol-puhe/data/filtered_data.csv")

# ggsave("logistic_gender_f0.png")

model_f0 <- glm(gender~f0, data = data)
summary(model_f0)

## Logistic regression ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## gender ~ f0 / f1

data$f0_over_f1 <- data$f0 / data$f1

ggplot(data, aes(x=f0_over_f1, y=gender)) +
  geom_point(alpha=.5, aes(color = cut(data$gender, c(-1, 0.5, 1)))) +
  scale_color_manual(name = "Gender",
                     values = c("(-1,0.5]" = "#ff0080",
                                "(0.5,1]" = "#0000ff"),
                     labels = c("Female", "Male")) +
  stat_smooth(method="glm", se=FALSE,
              method.args = list(family=binomial), col="#a349a4") +
  labs(title = "Logistic regression",
       subtitle = "gender ~ f0 / f1",
       caption = "Data: smol-puhe/data/filtered_data.csv") + 
  xlab("f0 / f1")

# ggsave("logistic_gender_f0_over_f1.png")

model_f0_over_f1 <- glm(gender~f0_over_f1, data = data, family="binomial")
summary(model_f0_over_f1)

## Logistic regression ########################################################
## gender ~ f1 / f0

data$f1_over_f0 = data$f1 / data$f0

ggplot(data, aes(x=f1_over_f0, y=gender)) +
  geom_point(alpha=.5, aes(color = cut(data$gender, c(-1, 0.5, 1)))) +
  scale_color_manual(name = "Gender",
                     values = c("(-1,0.5]" = "#ff0080",
                                "(0.5,1]" = "#0000ff"),
                     labels = c("Female", "Male")) +
  stat_smooth(method="glm", se=FALSE,
              method.args = list(family=binomial), 
              col="#a349a4") + 
  labs(title = "Logistic regression",
       subtitle = "gender ~ f1 / f0",
       caption = "Data: smol-puhe/data/filtered_data.csv") + 
  xlab("f1 / f0")

# ggsave("logistic_gender_f1_over_f0.png")

model_f1_over_f0 <- glm(gender~f1_over_f0, data = data, family="binomial")
summary(model_f1_over_f0)

## Logistic regression ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
## gender ~ f1 - f0

data$f1_minus_f0 = data$f1 - data$f0

ggplot(data, aes(x=f1_minus_f0, y=gender)) +
  geom_point(alpha=.5, aes(color = cut(data$gender, c(-1, 0.5, 1)))) +
  scale_color_manual(name = "Gender",
                     values = c("(-1,0.5]" = "#ff0080",
                                "(0.5,1]" = "#0000ff"),
                     labels = c("Female", "Male")) +
  stat_smooth(method="glm", se=FALSE,
              method.args = list(family=binomial), 
              col="#a349a4") + 
  labs(title = "Logistic regression",
       subtitle = "gender ~ f1 - f0",
       caption = "Data: smol-puhe/data/filtered_data.csv") + 
  xlab("f1 - f0")

# ggsave("logistic_gender_f1_minus_f0.png")

model_f1_minus_f0 <- glm(gender~f1_minus_f0, data = data, family="binomial")
summary(model_f1_minus_f0)

## Logistic regression ########################################################
## gender ~ f0 + f1

ggplot(data, aes(x=f0 + f1, y=gender)) +
  geom_point(alpha=.5, aes(color = cut(data$gender, c(-1, 0.5, 1)))) +
  scale_color_manual(name = "Gender",
                     values = c("(-1,0.5]" = "#ff0080",
                                "(0.5,1]" = "#0000ff"),
                     labels = c("Female", "Male")) +
  stat_smooth(method="glm", se=FALSE,
              method.args = list(family=binomial), 
              col="#a349a4") + 
  labs(title = "Logistic regression",
       subtitle = "gender ~ f1 + f0",
       caption = "Data: smol-puhe/data/filtered_data.csv") + 
  xlab("f0, f1")

# ggsave("logistic_gender_f1_minus_f0.png")

model <- glm(gender~f0 + f1 + f0*f1, data = data, family="binomial")
summary(model)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
